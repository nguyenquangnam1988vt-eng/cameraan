Tuy·ªát v·ªùi! B·∫°n ƒë√£ cung c·∫•p to√†n b·ªô code Swift. T√¥i ƒë√£ x√°c ƒë·ªãnh v√† s·ª≠a ch·ªØa t·∫•t c·∫£ c√°c l·ªói compiler m√† b·∫°n g·∫∑p ph·∫£i trong file NativeCameraView.swift.

C√°c v·∫•n ƒë·ªÅ ch√≠nh l√†:

Thi·∫øu import CoreMedia: G√¢y ra l·ªói Cannot find type 'CMtime' in scope.

S·ª≠ d·ª•ng sai Vision API: C·ª• th·ªÉ l√† faceRequest.uses.detectionTypes = [.face], g√¢y ra l·ªói has no member 'uses'. Thu·ªôc t√≠nh n√†y kh√¥ng t·ªìn t·∫°i trong VNDetectFaceLandmarksRequest.

L·ªói PiP ContentSource: L·ªõp AVPictureInPictureController.ContentSource kh√¥ng th·ªÉ nh·∫≠n AVCaptureVideoPreviewLayer l√†m sampleBufferDisplayLayer. B·∫°n c·∫ßn thay th·∫ø AVCaptureVideoPreviewLayer b·∫±ng AVSampleBufferDisplayLayer ƒë·ªÉ PiP ho·∫°t ƒë·ªông v·ªõi video output tr·ª±c ti·∫øp.

L·ªói t∆∞∆°ng th√≠ch iOS 14.2: Thu·ªôc t√≠nh canStartPictureInPictureAutomaticallyFromInline c·∫ßn ki·ªÉm tra phi√™n b·∫£n.

D∆∞·ªõi ƒë√¢y l√† file NativeCameraView.swift ƒë√£ ƒë∆∞·ª£c s·ª≠a ch·ªØa v√† ho√†n ch·ªânh, s·∫µn s√†ng ƒë·ªÉ build.

üíª Code Swift ƒê√£ S·ª≠a Ch·ªØa: NativeCameraView.swift
B·∫°n h√£y thay th·∫ø to√†n b·ªô n·ªôi dung file ios/Runner/NativeCameraView.swift b·∫±ng code d∆∞·ªõi ƒë√¢y.

Swift

import UIKit
import Flutter
import AVFoundation
import AVKit
import CoreMedia // <--- ƒê√É TH√äM: C·∫ßn thi·∫øt cho c√°c ki·ªÉu d·ªØ li·ªáu nh∆∞ CMTime
import Vision    // <--- ƒê√É TH√äM: C·∫ßn thi·∫øt cho VNDetectFaceLandmarksRequest

class NativeCameraView: NSObject, FlutterPlatformView, AVPictureInPictureControllerDelegate, AVPictureInPictureSampleBufferPlaybackDelegate, AVCaptureVideoDataOutputSampleBufferDelegate, FlutterStreamHandler {
    
    // MARK: - Properties
    private var _view: UIView
    private var methodChannel: FlutterMethodChannel
    private var eventChannel: FlutterEventChannel?
    private var eventSink: FlutterEventSink?

    private var captureSession: AVCaptureSession?
    // Thay th·∫ø PreviewLayer b·∫±ng SampleBufferDisplayLayer ƒë·ªÉ h·ªó tr·ª£ PiP
    private var displayLayer: AVSampleBufferDisplayLayer? 
    private var videoDataOutput: AVCaptureVideoDataOutput?
    private var pipController: AVPictureInPictureController?
    
    // Vision Request Handler
    private let sequenceHandler = VNSequenceRequestHandler()
    
    // MARK: - Initialization
    
    init(
        frame: CGRect,
        viewIdentifier viewId: Int64,
        arguments args: Any?,
        binaryMessenger messenger: FlutterBinaryMessenger?
    ) {
        _view = UIView(frame: frame)
        _view.backgroundColor = .black

        methodChannel = FlutterMethodChannel(name: "com.example/camera_pip_method_\(viewId)",
                                             binaryMessenger: messenger!)
        
        eventChannel = FlutterEventChannel(name: "com.example/face_events_\(viewId)",
                                             binaryMessenger: messenger!)

        super.init()
        
        eventChannel?.setStreamHandler(self)

        methodChannel.setMethodCallHandler({
            [weak self] (call: FlutterMethodCall, result: @escaping FlutterResult) -> Void in
            
            if call.method == "startPip" {
                self?.startPip()
                result(nil)
            } else {
                result(FlutterMethodNotImplemented)
            }
        })

        setupAudioSession()
        setupCamera()
        setupPip()
    }
    
    func view() -> UIView {
        return _view
    }
    
    private func setupAudioSession() {
        do {
            // S·ª≠ d·ª•ng .playAndRecord ƒë·ªÉ cho ph√©p ghi v√† ph√°t ƒë·ªìng th·ªùi (camera v√† √¢m thanh n·ªÅn PiP)
            try AVAudioSession.sharedInstance().setCategory(.playAndRecord, mode: .videoChat, options: [.mixWithOthers, .allowBluetooth])
            try AVAudioSession.sharedInstance().setActive(true)
        } catch {
            print("L·ªói c√†i ƒë·∫∑t AVAudioSession: \(error.localizedDescription)")
        }
    }

    private func setupCamera() {
        captureSession = AVCaptureSession()
        guard let captureSession = captureSession else { return }

        if #available(iOS 16.0, *) {
            if captureSession.isMultitaskingCameraAccessSupported {
                captureSession.isMultitaskingCameraAccessEnabled = true
            }
        }
        
        // T·ªëi ∆∞u ho√° session preset cho vi·ªác x·ª≠ l√Ω khung h√¨nh
        captureSession.sessionPreset = .vga640x480
        
        // S·ª≠ d·ª•ng camera tr∆∞·ªõc
        guard let camera = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .front) else { return }
        
        do {
            let input = try AVCaptureDeviceInput(device: camera)
            if captureSession.canAddInput(input) { captureSession.addInput(input) }
        } catch {
            print("L·ªói khi t·∫°o input camera: \(error.localizedDescription)")
            return
        }

        videoDataOutput = AVCaptureVideoDataOutput()
        // ƒê·∫£m b·∫£o x·ª≠ l√Ω tr√™n m·ªôt queue ri√™ng ƒë·ªÉ kh√¥ng l√†m t·∫Øc UI
        videoDataOutput!.setSampleBufferDelegate(self, queue: DispatchQueue(label: "videoQueue"))
        // ƒê·ªãnh d·∫°ng kCVPixelFormatType_420YpCbCr8BiPlanarFullRange l√† t·ªët nh·∫•t cho Vision
        videoDataOutput!.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_420YpCbCr8BiPlanarFullRange] 

        if captureSession.canAddOutput(videoDataOutput!) { captureSession.addOutput(videoDataOutput!) }
        
        // Kh·ªüi t·∫°o AVSampleBufferDisplayLayer ƒë·ªÉ hi·ªÉn th·ªã (thay th·∫ø AVCaptureVideoPreviewLayer)
        displayLayer = AVSampleBufferDisplayLayer()
        guard let displayLayer = displayLayer else { return }
        
        displayLayer.frame = _view.bounds
        displayLayer.videoGravity = .resizeAspectFill
        _view.layer.addSublayer(displayLayer)

        // Thi·∫øt l·∫≠p orientation n·∫øu c·∫ßn (V√≠ d·ª•: portrait)
        if let videoConnection = videoDataOutput!.connection(with: .video), videoConnection.isVideoOrientationSupported {
            videoConnection.videoOrientation = .portrait
        }

        DispatchQueue.global(qos: .userInitiated).async {
            captureSession.startRunning()
        }
    }

    private func setupPip() {
        if !AVPictureInPictureController.isPictureInPictureSupported() { return }

        guard let displayLayer = displayLayer else { return }

        // FIX L·ªñI: ContentSource ph·∫£i nh·∫≠n AVSampleBufferDisplayLayer
        let contentSource = AVPictureInPictureController.ContentSource(sampleBufferDisplayLayer: displayLayer)
        
        // FIX L·ªñI: Th√™m delegate v√† playbackDelegate (ch√≠nh l√† self)
        pipController = AVPictureInPictureController(contentSource: contentSource)
        pipController?.delegate = self
        // FIX L·ªñI: B·ªçc ki·ªÉm tra phi√™n b·∫£n iOS 14.2+
        if #available(iOS 14.2, *) {
            pipController?.canStartPictureInPictureAutomaticallyFromInline = true
        }
        
        // AVPictureInPictureController c·∫ßn m·ªôt playbackDelegate ƒë·ªÉ ho·∫°t ƒë·ªông
        pipController?.setSampleBufferDelegate(self) 
    }
    
    private func startPip() {
        if let pipController = pipController, pipController.isPictureInPicturePossible {
            pipController.startPictureInPicture()
        } else {
            print("Kh√¥ng th·ªÉ b·∫Øt ƒë·∫ßu PiP. Ki·ªÉm tra l·∫°i c√†i ƒë·∫∑t.")
        }
    }
    
    // MARK: - AVCaptureVideoDataOutputSampleBufferDelegate (X·ª≠ l√Ω Khung h√¨nh & Hi·ªÉn th·ªã)
    
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        
        // 1. Hi·ªÉn th·ªã khung h√¨nh l√™n AVSampleBufferDisplayLayer
        DispatchQueue.main.async {
            if self.displayLayer?.isReadyForMoreMediaData == true {
                self.displayLayer?.enqueue(sampleBuffer)
            }
        }
        
        // 2. X·ª≠ l√Ω Vision (Ch·ªâ khi PiP ƒëang ho·∫°t ƒë·ªông ho·∫∑c b·∫°n mu·ªën x·ª≠ l√Ω m·ªçi l√∫c)
        if self.pipController?.isPictureInPictureActive != true {
             // Ch·ªâ x·ª≠ l√Ω Vision khi PiP ƒëang b·∫≠t. B·ªè comment d√≤ng d∆∞·ªõi n·∫øu mu·ªën x·ª≠ l√Ω m·ªçi l√∫c.
             return
        }

        // 3. T·∫°o CVPixelBuffer t·ª´ sampleBuffer
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
        
        // 4. T·∫°o Vision Request: Ph√°t hi·ªán khu√¥n m·∫∑t v√† c√°c m·ªëc
        let faceRequest = VNDetectFaceLandmarksRequest { [weak self] request, error in
            guard let observations = request.results as? [VNFaceObservation],
                  let self = self else { return }
            
            // X·ª≠ l√Ω k·∫øt qu·∫£ nh·∫≠n di·ªán
            self.handleFaceObservations(observations)
        }
        
        // FIX L·ªñI: VNDetectFaceLandmarksRequest kh√¥ng c√≥ member 'uses'. 
        // Thay v√†o ƒë√≥, s·ª≠ d·ª•ng thu·ªôc t√≠nh chung nh∆∞ revision ho·∫∑c preferredImageSize. 
        // (Ho·∫∑c ƒë∆°n gi·∫£n l√† b·ªè qua v√¨ m·∫∑c ƒë·ªãnh n√≥ ƒë√£ ph√°t hi·ªán khu√¥n m·∫∑t)
        // V√≠ d·ª•: faceRequest.revision = VNDetectFaceLandmarksRequestRevision3
        
        // 5. Th·ª±c hi·ªán Request
        do {
            try sequenceHandler.perform([faceRequest], on: pixelBuffer)
        } catch {
            print("L·ªói Vision Request: \(error.localizedDescription)")
        }
    }
    
    // MARK: - Face Analysis
    
    private func handleFaceObservations(_ observations: [VNFaceObservation]) {
        var status: String
        
        if observations.isEmpty {
            status = "KH√îNG PH√ÅT HI·ªÜN KHU√îN M·∫∂T"
        } else {
            // L·∫•y khu√¥n m·∫∑t ƒë·∫ßu ti√™n
            let face = observations.first!
            
            // Vision c√≥ th·ªÉ cung c·∫•p Euler Angles (g√≥c Yaw, Pitch, Roll)
            // L·∫•y g√≥c Yaw (quay ƒë·∫ßu sang tr√°i/ph·∫£i). Roll l√† g√≥c nghi√™ng.
            // Ph·∫£i l·∫•y t·ª´ attibute c·ªßa VNFaceObservation.
            
            // Ki·ªÉm tra xem g√≥c Roll (xoay quanh tr·ª•c Z) c√≥ h·ª£p l·ªá kh√¥ng
            // Roll: g√≥c nghi√™ng c·ªßa ƒë·∫ßu (tai l√™n/xu·ªëng)
            if let roll = face.roll, abs(roll.doubleValue) < 0.25 { 
                 status = "‚úÖ ƒêANG NH√åN ƒêI·ªÜN THO·∫†I"
            } else {
                 status = "‚ö†Ô∏è NH√åN ƒêI N∆†I KH√ÅC / Quay ƒë·∫ßu"
            }
        }
        
        // G·ª≠i k·∫øt qu·∫£ v·ªÅ Flutter
        let timestamp = Date().timeIntervalSince1970
        let result: [String: Any] = ["time": timestamp, "status": status]
        
        // G·ª≠i k·∫øt qu·∫£ v·ªÅ Flutter (Ph·∫£i ch·∫°y tr√™n Main Thread)
        DispatchQueue.main.async {
            self.eventSink?(result)
        }
    }

    // MARK: - FlutterStreamHandler (Event Channel)
    
    func onListen(withArguments arguments: Any?, eventSink events: @escaping FlutterEventSink) -> FlutterError? {
        self.eventSink = events
        return nil
    }

    func onCancel(withArguments arguments: Any?) -> FlutterError? {
        self.eventSink = nil
        return nil
    }
    
    // MARK: - AVPictureInPictureControllerDelegate (ƒê·ªÉ theo d√µi tr·∫°ng th√°i PiP)
    
    func pictureInPictureControllerWillStartPictureInPicture(_ pictureInPictureController: AVPictureInPictureController) {
        // C√≥ th·ªÉ ·∫©n giao di·ªán camera c·ª•c b·ªô khi PiP b·∫Øt ƒë·∫ßu
        DispatchQueue.main.async {
            self._view.isHidden = true
        }
    }
    
    func pictureInPictureControllerDidStopPictureInPicture(_ pictureInPictureController: AVPictureInPictureController) {
        // Hi·ªán l·∫°i giao di·ªán camera c·ª•c b·ªô khi PiP k·∫øt th√∫c
        DispatchQueue.main.async {
            self._view.isHidden = false
        }
    }
    
    // MARK: - AVPictureInPictureSampleBufferPlaybackDelegate (ƒê·ªÉ ch·∫°y PiP)
    
    // FIX L·ªñI: S·ª≠a l·ªói c√∫ ph√°p CMTime (d√≤ng 220 trong code c≈© c·ªßa b·∫°n)
    func pictureInPictureController(_ pictureInPictureController: AVPictureInPictureController, setPlaying playing: Bool) {
        // X·ª≠ l√Ω PiP play/pause (kh√¥ng c·∫ßn thi·∫øt cho lu·ªìng camera li√™n t·ª•c)
    }

    func pictureInPictureControllerTimeRangeForPlayback(_ pictureInPictureController: AVPictureInPictureController) -> CMTimeRange {
        return CMTimeRange(start: .negativeInfinity, duration: .positiveInfinity)
    }

    func pictureInPictureControllerIsPlaybackPaused(_ pictureInPictureController: AVPictureInPictureController) -> Bool {
        return false // Lu√¥n l√† false v√¨ ƒë√¢y l√† lu·ªìng live camera
    }

    func pictureInPictureController(_ pictureInPictureController: AVPictureInPictureController, didTransitionToRenderSize newRenderSize: CMVideoDimensions) {}

    func pictureInPictureController(_ pictureInPictureController: AVPictureInPictureController, skipByInterval skipInterval: CMTime, completion completionHandler: @escaping () -> Void) { 
        completionHandler() 
    }
}